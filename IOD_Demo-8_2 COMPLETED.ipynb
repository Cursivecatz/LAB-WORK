{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEahhQs1plFz"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjoKu00cplF5"
   },
   "source": [
    "# Demo 8.2: Bagging\n",
    "\n",
    "INSTRUCTIONS:\n",
    "\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgmr_-6cplGN"
   },
   "source": [
    "This is an excerpt from the [Ensemble Learning to Improve Machine Learning Results-How ensemble methods work: bagging, boosting and stacking](https://blog.statsbot.co/ensemble-learning-d1dcd548e936) by **Vadim Smolyakov**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1I_iXDq0plGR"
   },
   "source": [
    "## Bagging\n",
    "**Bagging** stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train $M$ different trees $f_m$ on different subsets of the data (chosen randomly with replacement) and compute the ensemble:\n",
    "\n",
    "$$\n",
    "   f(x) = \\frac{1}{M}\\sum_{m=1}^{M}f_m(x) \n",
    "$$\n",
    "\n",
    "Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from mlxtend) (3.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from mlxtend) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from mlxtend) (1.19.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from mlxtend) (50.3.1.post20201107)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from mlxtend) (0.17.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\para raja singham\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eEnQT38plGS"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9oRR2IaplGj"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FeT85llYplGs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading the dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# picking just the first two features\n",
    "X = iris.data[:, 0:2]\n",
    "# target\n",
    "y = iris.target\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5],\n",
       "       [4.9, 3. ],\n",
       "       [4.7, 3.2],\n",
       "       [4.6, 3.1],\n",
       "       [5. , 3.6],\n",
       "       [5.4, 3.9],\n",
       "       [4.6, 3.4],\n",
       "       [5. , 3.4],\n",
       "       [4.4, 2.9],\n",
       "       [4.9, 3.1],\n",
       "       [5.4, 3.7],\n",
       "       [4.8, 3.4],\n",
       "       [4.8, 3. ],\n",
       "       [4.3, 3. ],\n",
       "       [5.8, 4. ],\n",
       "       [5.7, 4.4],\n",
       "       [5.4, 3.9],\n",
       "       [5.1, 3.5],\n",
       "       [5.7, 3.8],\n",
       "       [5.1, 3.8],\n",
       "       [5.4, 3.4],\n",
       "       [5.1, 3.7],\n",
       "       [4.6, 3.6],\n",
       "       [5.1, 3.3],\n",
       "       [4.8, 3.4],\n",
       "       [5. , 3. ],\n",
       "       [5. , 3.4],\n",
       "       [5.2, 3.5],\n",
       "       [5.2, 3.4],\n",
       "       [4.7, 3.2],\n",
       "       [4.8, 3.1],\n",
       "       [5.4, 3.4],\n",
       "       [5.2, 4.1],\n",
       "       [5.5, 4.2],\n",
       "       [4.9, 3.1],\n",
       "       [5. , 3.2],\n",
       "       [5.5, 3.5],\n",
       "       [4.9, 3.6],\n",
       "       [4.4, 3. ],\n",
       "       [5.1, 3.4],\n",
       "       [5. , 3.5],\n",
       "       [4.5, 2.3],\n",
       "       [4.4, 3.2],\n",
       "       [5. , 3.5],\n",
       "       [5.1, 3.8],\n",
       "       [4.8, 3. ],\n",
       "       [5.1, 3.8],\n",
       "       [4.6, 3.2],\n",
       "       [5.3, 3.7],\n",
       "       [5. , 3.3],\n",
       "       [7. , 3.2],\n",
       "       [6.4, 3.2],\n",
       "       [6.9, 3.1],\n",
       "       [5.5, 2.3],\n",
       "       [6.5, 2.8],\n",
       "       [5.7, 2.8],\n",
       "       [6.3, 3.3],\n",
       "       [4.9, 2.4],\n",
       "       [6.6, 2.9],\n",
       "       [5.2, 2.7],\n",
       "       [5. , 2. ],\n",
       "       [5.9, 3. ],\n",
       "       [6. , 2.2],\n",
       "       [6.1, 2.9],\n",
       "       [5.6, 2.9],\n",
       "       [6.7, 3.1],\n",
       "       [5.6, 3. ],\n",
       "       [5.8, 2.7],\n",
       "       [6.2, 2.2],\n",
       "       [5.6, 2.5],\n",
       "       [5.9, 3.2],\n",
       "       [6.1, 2.8],\n",
       "       [6.3, 2.5],\n",
       "       [6.1, 2.8],\n",
       "       [6.4, 2.9],\n",
       "       [6.6, 3. ],\n",
       "       [6.8, 2.8],\n",
       "       [6.7, 3. ],\n",
       "       [6. , 2.9],\n",
       "       [5.7, 2.6],\n",
       "       [5.5, 2.4],\n",
       "       [5.5, 2.4],\n",
       "       [5.8, 2.7],\n",
       "       [6. , 2.7],\n",
       "       [5.4, 3. ],\n",
       "       [6. , 3.4],\n",
       "       [6.7, 3.1],\n",
       "       [6.3, 2.3],\n",
       "       [5.6, 3. ],\n",
       "       [5.5, 2.5],\n",
       "       [5.5, 2.6],\n",
       "       [6.1, 3. ],\n",
       "       [5.8, 2.6],\n",
       "       [5. , 2.3],\n",
       "       [5.6, 2.7],\n",
       "       [5.7, 3. ],\n",
       "       [5.7, 2.9],\n",
       "       [6.2, 2.9],\n",
       "       [5.1, 2.5],\n",
       "       [5.7, 2.8],\n",
       "       [6.3, 3.3],\n",
       "       [5.8, 2.7],\n",
       "       [7.1, 3. ],\n",
       "       [6.3, 2.9],\n",
       "       [6.5, 3. ],\n",
       "       [7.6, 3. ],\n",
       "       [4.9, 2.5],\n",
       "       [7.3, 2.9],\n",
       "       [6.7, 2.5],\n",
       "       [7.2, 3.6],\n",
       "       [6.5, 3.2],\n",
       "       [6.4, 2.7],\n",
       "       [6.8, 3. ],\n",
       "       [5.7, 2.5],\n",
       "       [5.8, 2.8],\n",
       "       [6.4, 3.2],\n",
       "       [6.5, 3. ],\n",
       "       [7.7, 3.8],\n",
       "       [7.7, 2.6],\n",
       "       [6. , 2.2],\n",
       "       [6.9, 3.2],\n",
       "       [5.6, 2.8],\n",
       "       [7.7, 2.8],\n",
       "       [6.3, 2.7],\n",
       "       [6.7, 3.3],\n",
       "       [7.2, 3.2],\n",
       "       [6.2, 2.8],\n",
       "       [6.1, 3. ],\n",
       "       [6.4, 2.8],\n",
       "       [7.2, 3. ],\n",
       "       [7.4, 2.8],\n",
       "       [7.9, 3.8],\n",
       "       [6.4, 2.8],\n",
       "       [6.3, 2.8],\n",
       "       [6.1, 2.6],\n",
       "       [7.7, 3. ],\n",
       "       [6.3, 3.4],\n",
       "       [6.4, 3.1],\n",
       "       [6. , 3. ],\n",
       "       [6.9, 3.1],\n",
       "       [6.7, 3.1],\n",
       "       [6.9, 3.1],\n",
       "       [5.8, 2.7],\n",
       "       [6.8, 3.2],\n",
       "       [6.7, 3.3],\n",
       "       [6.7, 3. ],\n",
       "       [6.3, 2.5],\n",
       "       [6.5, 3. ],\n",
       "       [6.2, 3.4],\n",
       "       [5.9, 3. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1v_8m5SJplG0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "[[5.1 3.5]\n",
      " [4.9 3. ]\n",
      " [4.7 3.2]\n",
      " [4.6 3.1]\n",
      " [5.  3.6]]\n"
     ]
    }
   ],
   "source": [
    "## Check the data\n",
    "\n",
    "# About data\n",
    "print(X.shape)\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SKSnRxXEplG4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# About target\n",
    "print(y.shape)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1bbhUQ_plG6"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsOKFnygplG7"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "clf1 = DecisionTreeClassifier(criterion = 'entropy', max_depth = 2)\n",
    "clf2 = KNeighborsClassifier(n_neighbors = 1)    \n",
    "\n",
    "bagging1 = BaggingClassifier(\n",
    "    base_estimator = clf1,\n",
    "    n_estimators = 10,\n",
    "    max_samples = 0.8,\n",
    "    max_features = 0.8)\n",
    "bagging2 = BaggingClassifier(\n",
    "    base_estimator = clf2,\n",
    "    n_estimators = 10,\n",
    "    max_samples = 0.8,\n",
    "    max_features = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALsyfnj8plHA"
   },
   "source": [
    "## Presenting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HiiqN1__plHM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.04) [Decision Tree]\n",
      "Accuracy: 0.70 (+/- 0.02) [K-NN]\n"
     ]
    }
   ],
   "source": [
    "label = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']\n",
    "clf_list = [clf1, clf2, bagging1, bagging2]\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0, 1], repeat = 2)\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv = 3, scoring = 'accuracy')\n",
    "    print('Accuracy: %.2f (+/- %.2f) [%s]' % (scores.mean(), scores.std(), label))\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X = X, y = y, clf = clf, legend = 2)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1DeDRwaplHP"
   },
   "source": [
    "The figure above shows the decision boundary of a decision tree and k-NN classifiers along with their bagging ensembles applied to the Iris dataset. The decision tree shows axes parallel boundaries while the $k=1$ nearest neighbors fits closely to the data points. The bagging ensembles were trained using $10$ base estimators with $0.8$ subsampling of training data and $0.8$ subsampling of features. The decision tree bagging ensemble achieved higher accuracy in comparison to k-NN bagging ensemble because k-NN are less sensitive to perturbation on training samples and therefore they are called *stable learners*. Combining stable learners is less advantageous since the ensemble will not help improve generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pK_7GpRXplHQ"
   },
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "np.random.seed(2534)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, bagging1, print_model = False, style = 'ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPfsHs-FplHm"
   },
   "source": [
    "The figure above shows learning curves for the bagging tree ensemble. We can see an average error of $0.3$ on the training data and a U-shaped error curve for the testing data. The smallest gap between training and test errors occurs at around $80\\%$ of the training set size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45RUq53gplHo"
   },
   "outputs": [],
   "source": [
    "# Ensemble Size\n",
    "num_est = np.linspace(1, 100, 20, dtype = np.int8)\n",
    "\n",
    "np.random.seed(2534)\n",
    "\n",
    "bg_clf_cv_mean = []\n",
    "bg_clf_cv_std = []\n",
    "for n_est in num_est:    \n",
    "    bg_clf = BaggingClassifier(\n",
    "        base_estimator = clf1,\n",
    "        n_estimators = n_est,\n",
    "        max_samples = 0.8,\n",
    "        max_features = 0.8)\n",
    "    scores = cross_val_score(bg_clf, X, y, cv = 3, scoring = 'accuracy')\n",
    "    bg_clf_cv_mean.append(scores.mean())\n",
    "    bg_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHoCjSX6plH3"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(\n",
    "    num_est,\n",
    "    bg_clf_cv_mean,\n",
    "    yerr = bg_clf_cv_std,\n",
    "    c = 'blue',\n",
    "    fmt = '-o',\n",
    "    capsize = 5)\n",
    "\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "\n",
    "plt.title('Bagging Tree Ensemble')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Ensemble Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yToJAOIplI7"
   },
   "source": [
    "The figure above shows how the test accuracy improves with the size of the ensemble. Based on cross-validation results, we can see the accuracy increases until approximately $10$ base estimators and then plateaus afterwards. Thus, adding base estimators beyond $10$ only increases computational complexity without accuracy gains for the Iris dataset.\n",
    "\n",
    "A commonly used class of ensemble algorithms are forests of randomized trees. In **random forests**, each tree in the ensemble is built from a sample drawn with replacement (i.e. a bootstrap sample) from the training set. In addition, instead of using all the features, a random subset of features is selected further randomizing the tree. As a result, the bias of the forest increases slightly but due to averaging of less correlated trees, its variance decreases resulting in an overall better model.\n",
    "\n",
    "In **extremely randomized trees** algorithm randomness goes one step further: the splitting thresholds are randomized. Instead of looking for the most discriminative threshold, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2021 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Demo-8_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
